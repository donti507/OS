Laboratory tasks involve the simulation and study of the properties of algorithms/mechanisms used in 
operating systems. Consequently, the basis for passing such a task is not only the software but also 
knowledge of the relevant issues, verified in the form of an oral answer. 
Task 1
The task is to implement a simulation of the processor scheduling algorithms performance.
- FCFS,
- SJF (or SRTF),
- Round-Robin (with choice of time quantum).
Formulate the simulation assumptions independently. 
Tips: 
- Algorithms should be tested at the same data (i.e. the same test sequences of processes),
- There should be more test sequences (20? 50?); the result will be average values,
- In each sequence there will be N processes with random processor phase lengths (the 
distribution of phase lengths should be chosen to correspond to the situation in the real 
system, where it is not uniform), reporting at random times (choose the parameters so that a 
queue of processes waiting for processor allocation can be formed),
- Possible process representation: number, length of processor phase, moment of request,
waiting time,
- The possibility to control simulation parameters is welcome. 
Task 2 
Simulation of HDD scheduling algorithms. 
- Disk is in our case a linearly ordered sequence of blocks numbered from 1 to MAX,
- The criterion for evaluating the algorithms will be the sum of disk head movements, as it is 
known, proportional to the execution time of the jobs, 
- Check the FCFS, SSTF, SCAN and C-SCAN algorithms,
- Then assume that there are also real-time applications in the system that need to be served 
by EDF and/or FD-SCAN. How does this affect the results? 
Tips:
It is up to you to formulate the simulation conditions not mentioned above. It means: 
- The size of the disk (number of blocks),
- Number and generation of requests (full queue from the beginning? submissions in progress? 
distribution of requests - even, different?),
- Real-time requests handling,
- Ability to justify the solution adopted would be appreciated.
Task 3 
Investigation of page replacement algorithms. 
Formulate the assumptions of the simulation yourself: 
- Virtual memory size (number of pages),
- Size of physical memory (number of frames),
- Length (should be significant - min. 1000) and how the sequence of page references is 
generated (it is necessary to take into account the principle of locality of references). 
Plan: 
- Generate a random sequence of n page references,
- For the generated sequence provide the number of page faults for different page replacement 
algorithms: 
- FIFO 
- OPT
- LRU
- approximated LRU
- RAND (we remove a random page) 
Task 4 
Progressive complication of Task 4:
- there are a number (of the order of ~10) processes running on the system,
- each uses its own set of pages (locality principle still applies),
- the global sequence of references is the result of combining a sequence of references 
generated by individual processes (each generates many, not one)
- each system allocates a certain number of frames. based on the following methods: 
o Proportional allocation 
o Equal allocation 
o Page-Fault Frequency
o Working-set model. 
- Page substitution is done according to LRU. 
- How do the frame allocation strategies affect the results (number of page errors - globally, for 
each process)? 
Task 5 
Simulation of distributed load balancing
There are N identical processors in the system. On each of them new tasks (processes) appear, with 
DIFFERENT frequencies and DIFFERENT requirements (each process requires a certain, different
processor power share - e.g. ~3%). 
A task appears on processor x. Simulate the following allocation strategies
- x asks a randomly selected processor y for the current load. If it is less than threshold p, the 
process is sent there. If not, we draw new processor and ask the next one, trying at most z 
times. If all drawn processor are loaded above p, the process executes on x,
- If the load on x exceeds the threshold value p, the process is sent to a randomly selected 
processor y with a load less than p (if the drawn y has load>p, the draw is repeated until 
successful). If it does not exceed - the process executes on x,
- As in point 2, except that processors with load less than the minimum threshold r ask randomly 
selected processors and if the load of the asked processor is greater than p, the asking 
processor takes over some of its tasks (assume which one).
Simulate strategies 1-3 for N= about 50-100 and a long series of tasks to be performed. In each case 
give as result: 
- Average CPU load (decide, reasonably, how it will be calculated),
- Average deviation from the value in previous point,
- Number of load queries and process migrations (moves).Laboratory tasks involve the simulation and study of the properties of algorithms/mechanisms used in 
operating systems. Consequently, the basis for passing such a task is not only the software but also 
knowledge of the relevant issues, verified in the form of an oral answer. 
Task 1
The task is to implement a simulation of the processor scheduling algorithms performance.
- FCFS,
- SJF (or SRTF),
- Round-Robin (with choice of time quantum).
Formulate the simulation assumptions independently. 
Tips: 
- Algorithms should be tested at the same data (i.e. the same test sequences of processes),
- There should be more test sequences (20? 50?); the result will be average values,
- In each sequence there will be N processes with random processor phase lengths (the 
distribution of phase lengths should be chosen to correspond to the situation in the real 
system, where it is not uniform), reporting at random times (choose the parameters so that a 
queue of processes waiting for processor allocation can be formed),
- Possible process representation: number, length of processor phase, moment of request,
waiting time,
- The possibility to control simulation parameters is welcome. 
Task 2 
Simulation of HDD scheduling algorithms. 
- Disk is in our case a linearly ordered sequence of blocks numbered from 1 to MAX,
- The criterion for evaluating the algorithms will be the sum of disk head movements, as it is 
known, proportional to the execution time of the jobs, 
- Check the FCFS, SSTF, SCAN and C-SCAN algorithms,
- Then assume that there are also real-time applications in the system that need to be served 
by EDF and/or FD-SCAN. How does this affect the results? 
Tips:
It is up to you to formulate the simulation conditions not mentioned above. It means: 
- The size of the disk (number of blocks),
- Number and generation of requests (full queue from the beginning? submissions in progress? 
distribution of requests - even, different?),
- Real-time requests handling,
- Ability to justify the solution adopted would be appreciated.
Task 3 
Investigation of page replacement algorithms. 
Formulate the assumptions of the simulation yourself: 
- Virtual memory size (number of pages),
- Size of physical memory (number of frames),
- Length (should be significant - min. 1000) and how the sequence of page references is 
generated (it is necessary to take into account the principle of locality of references). 
Plan: 
- Generate a random sequence of n page references,
- For the generated sequence provide the number of page faults for different page replacement 
algorithms: 
- FIFO 
- OPT
- LRU
- approximated LRU
- RAND (we remove a random page) 
Task 4 
Progressive complication of Task 4:
- there are a number (of the order of ~10) processes running on the system,
- each uses its own set of pages (locality principle still applies),
- the global sequence of references is the result of combining a sequence of references 
generated by individual processes (each generates many, not one)
- each system allocates a certain number of frames. based on the following methods: 
o Proportional allocation 
o Equal allocation 
o Page-Fault Frequency
o Working-set model. 
- Page substitution is done according to LRU. 
- How do the frame allocation strategies affect the results (number of page errors - globally, for 
each process)? 
Task 5 
Simulation of distributed load balancing
There are N identical processors in the system. On each of them new tasks (processes) appear, with 
DIFFERENT frequencies and DIFFERENT requirements (each process requires a certain, different
processor power share - e.g. ~3%). 
A task appears on processor x. Simulate the following allocation strategies
- x asks a randomly selected processor y for the current load. If it is less than threshold p, the 
process is sent there. If not, we draw new processor and ask the next one, trying at most z 
times. If all drawn processor are loaded above p, the process executes on x,
- If the load on x exceeds the threshold value p, the process is sent to a randomly selected 
processor y with a load less than p (if the drawn y has load>p, the draw is repeated until 
successful). If it does not exceed - the process executes on x,
- As in point 2, except that processors with load less than the minimum threshold r ask randomly 
selected processors and if the load of the asked processor is greater than p, the asking 
processor takes over some of its tasks (assume which one).
Simulate strategies 1-3 for N= about 50-100 and a long series of tasks to be performed. In each case 
give as result: 
- Average CPU load (decide, reasonably, how it will be calculated),
- Average deviation from the value in previous point,
- Number of load queries and process migrations (moves).
